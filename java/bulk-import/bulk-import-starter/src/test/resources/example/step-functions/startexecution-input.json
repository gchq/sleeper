{
  "args": [
    "spark-submit",
    "--deploy-mode",
    "cluster",
    "--class",
    "sleeper.bulkimport.runner.dataframelocalsort.BulkImportDataframeLocalSortDriver",
    "--conf",
    "spark.hadoop.fs.s3a.experimental.input.fadvise=sequential",
    "--conf",
    "spark.rdd.compress=true",
    "--conf",
    "spark.network.timeout=800s",
    "--conf",
    "spark.sql.shuffle.partitions=290",
    "--conf",
    "spark.kubernetes.namespace=eks-namespace",
    "--conf",
    "spark.shuffle.spill.compress=true",
    "--conf",
    "spark.shuffle.compress=true",
    "--conf",
    "spark.default.parallelism=290",
    "--conf",
    "spark.executor.memoryOverhead=1g",
    "--conf",
    "spark.kubernetes.driver.pod.name=job-test-job",
    "--conf",
    "spark.kubernetes.authenticate.driver.serviceAccountName=spark",
    "--conf",
    "spark.hadoop.fs.s3a.aws.credentials.provider=software.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider",
    "--conf",
    "spark.executor.instances=3",
    "--conf",
    "spark.executor.memory=7g",
    "--conf",
    "spark.driver.memory=7g",
    "--conf",
    "spark.yarn.scheduler.reporterThread.maxFailures=5",
    "--conf",
    "spark.speculation.quantile=0.75",
    "--conf",
    "spark.master=k8s://null",
    "--conf",
    "spark.driver.cores=5",
    "--conf",
    "spark.executor.heartbeatInterval=60s",
    "--conf",
    "spark.executorEnv.JAVA_HOME=/opt/java/openjdk",
    "--conf",
    "spark.executor.cores=5",
    "--conf",
    "spark.memory.storageFraction=0.30",
    "--conf",
    "spark.app.name=test-job",
    "--conf",
    "spark.speculation=false",
    "--conf",
    "spark.shuffle.mapStatus.compression.codec=lz4",
    "--conf",
    "spark.kubernetes.container.image=test-account.dkr.ecr.test-region.amazonaws.com/test-instance/bulk-import-runner:1.2.3",
    "--conf",
    "spark.kubernetes.executor.podNamePrefix=job-test-job",
    "--conf",
    "spark.storage.level=MEMORY_AND_DISK_SER",
    "--conf",
    "spark.driver.memoryOverhead=1g",
    "--conf",
    "spark.hadoop.fs.s3a.connection.maximum=5",
    "--conf",
    "spark.memory.fraction=0.80",
    "--conf",
    "spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'",
    "--conf",
    "spark.dynamicAllocation.enabled=false",
    "--conf",
    "spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'",
    "local:///opt/spark/workdir/bulk-import-runner.jar",
    "config-bucket",
    "test-job",
    "state-machine-arn",
    "test-job-run",
    "EKS"
  ],
  "jobPodPrefix": "job-test-job",
  "job": {
    "id": "test-job",
    "tableName": "test-table",
    "tableId": "table-id",
    "files": [
      "file.parquet"
    ]
  }
}
